{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWPGMJEHNL4ZZ9gceUSZ2l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pexize/Cria-o-de-Uma-Base-de-Dados-e-Treinamento-da-Rede-YOLO/blob/main/rede%20yolo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WZI9Uzgip_o",
        "outputId": "ffed1dfb-70eb-4b6c-acd0-32b171038f77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'yolov3-tf2' already exists and is not an empty directory.\n",
            "/content/yolov3-tf2\n"
          ]
        }
      ],
      "source": [
        "# 1. Configurar o Ambiente\n",
        "# Não precisamos instalar o TensorFlow, pois ele já está instalado no Colab.\n",
        "!git clone https://github.com/zzh8829/yolov3-tf2.git\n",
        "%cd yolov3-tf2\n",
        "\n",
        "# Remover a linha do TensorFlow do requirements.txt\n",
        "!sed -i '/tensorflow/d' requirements.txt\n",
        "!sed -i 's/opencv-python==4.2.0.32/opencv-python/' requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar as dependências restantes\n",
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmnEjSELizjx",
        "outputId": "b79d5726-3010-4e20-9550-d3acb4d1b34b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/yolov3-tf2 (from -r requirements.txt (line 5))\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (4.10.0.84)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (5.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python->-r requirements.txt (line 1)) (1.26.4)\n",
            "Installing collected packages: yolov3_tf2\n",
            "  Attempting uninstall: yolov3_tf2\n",
            "    Found existing installation: yolov3_tf2 0.1\n",
            "    Uninstalling yolov3_tf2-0.1:\n",
            "      Successfully uninstalled yolov3_tf2-0.1\n",
            "  Running setup.py develop for yolov3_tf2\n",
            "Successfully installed yolov3_tf2-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from absl import flags\n",
        "from absl.flags import FLAGS\n",
        "import os\n",
        "import xml.etree.ElementTree as ET"
      ],
      "metadata": {
        "id": "juWWz9_Ci6H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Baixar um Dataset\n",
        "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
        "!tar -xvf VOCtrainval_06-Nov-2007.tar"
      ],
      "metadata": {
        "id": "YWJWy9qHi-11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir dataset\n",
        "!mv VOCdevkit/VOC2007/JPEGImages/* dataset/images\n",
        "!mv VOCdevkit/VOC2007/Annotations/* dataset/labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZQ2tyNijFGK",
        "outputId": "12c50a55-a423-4d42-f7d0-a15bfbb35fd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘dataset’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar se há imagens no diretório\n",
        "image_files = !ls dataset/images\n",
        "if not image_files:\n",
        "    raise FileNotFoundError(\"Nenhuma imagem encontrada em 'dataset/images'. Verifique o download e a organização dos dados.\")\n"
      ],
      "metadata": {
        "id": "P3uFTsY3yJhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Limitar o dataset a apenas algumas imagens para economizar espaço\n",
        "!ls dataset/images | head -n 50 > dataset/train.txt\n",
        "!ls dataset/images | tail -n 10 > dataset/val.txt"
      ],
      "metadata": {
        "id": "YZdEJf7UjcJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Verificar se os arquivos train.txt e val.txt foram criados\n",
        "if not os.path.exists('dataset/train.txt') or not os.path.exists('dataset/val.txt'):\n",
        "    raise FileNotFoundError(\"Os arquivos 'train.txt' ou 'val.txt' não foram criados. Verifique o diretório 'dataset/images'.\")"
      ],
      "metadata": {
        "id": "Vm0itkRQyQMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Preparar os Dados\n",
        "# Função para converter XML para TFRecord\n",
        "def create_tf_example(image_path, annotation_path, class_map):\n",
        "    # Ler a imagem\n",
        "    with tf.io.gfile.GFile(image_path, 'rb') as fid:\n",
        "        encoded_jpg = fid.read()\n",
        "\n",
        "    # Decodificar a imagem para obter as dimensões\n",
        "    image = tf.image.decode_image(encoded_jpg, channels=3)\n",
        "    image_shape = tf.shape(image)  # Obter as dimensões da imagem\n",
        "    height = image_shape[0].numpy()  # Altura\n",
        "    width = image_shape[1].numpy()   # Largura\n",
        "\n",
        "    # Ler o arquivo XML\n",
        "    tree = ET.parse(annotation_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Extrair informações das anotações\n",
        "    xmin = []\n",
        "    xmax = []\n",
        "    ymin = []\n",
        "    ymax = []\n",
        "    class_texts = []  # Nomes das classes (strings codificadas como bytes)\n",
        "    class_indices = []  # Índices das classes (inteiros)\n",
        "    for obj in root.findall('object'):\n",
        "        label = obj.find('name').text\n",
        "        if label not in class_map:\n",
        "            continue\n",
        "        class_texts.append(label.encode('utf8'))  # Nome da classe como bytes\n",
        "        class_indices.append(class_map[label])    # Índice da classe\n",
        "        bbox = obj.find('bndbox')\n",
        "        xmin.append(float(bbox.find('xmin').text) / width)\n",
        "        xmax.append(float(bbox.find('xmax').text) / width)\n",
        "        ymin.append(float(bbox.find('ymin').text) / height)\n",
        "        ymax.append(float(bbox.find('ymax').text) / height)\n",
        "\n",
        "    # Criar o exemplo TFRecord\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "        'image/height': tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
        "        'image/width': tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
        "        'image/filename': tf.train.Feature(bytes_list=tf.train.BytesList(value=[os.path.basename(image_path).encode('utf8')])),\n",
        "        'image/source_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[os.path.basename(image_path).encode('utf8')])),\n",
        "        'image/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[encoded_jpg])),\n",
        "        'image/format': tf.train.Feature(bytes_list=tf.train.BytesList(value=['jpeg'.encode('utf8')])),\n",
        "        'image/object/bbox/xmin': tf.train.Feature(float_list=tf.train.FloatList(value=xmin)),\n",
        "        'image/object/bbox/xmax': tf.train.Feature(float_list=tf.train.FloatList(value=xmax)),\n",
        "        'image/object/bbox/ymin': tf.train.Feature(float_list=tf.train.FloatList(value=ymin)),\n",
        "        'image/object/bbox/ymax': tf.train.Feature(float_list=tf.train.FloatList(value=ymax)),\n",
        "        'image/object/class/text': tf.train.Feature(bytes_list=tf.train.BytesList(value=class_texts)),  # Nomes das classes\n",
        "        'image/object/class/label': tf.train.Feature(int64_list=tf.train.Int64List(value=class_indices)),  # Índices das classes\n",
        "    }))\n",
        "    return tf_example"
      ],
      "metadata": {
        "id": "yPCrLUiSjHWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_tfrecord(data_dir, output_dir, split='train'):\n",
        "    # Mapeamento de classes\n",
        "    class_map = {name: idx + 1 for idx, name in enumerate(['aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n",
        "                                                           'bus', 'car', 'cat', 'chair', 'cow', 'diningtable',\n",
        "                                                           'dog', 'horse', 'motorbike', 'person', 'pottedplant',\n",
        "                                                           'sheep', 'sofa', 'train', 'tvmonitor'])}\n",
        "\n",
        "    # Ler lista de imagens\n",
        "    split_file = os.path.join(data_dir, f'{split}.txt')\n",
        "    with open(split_file, 'r') as f:\n",
        "        image_names = [line.strip().split('.')[0] for line in f.readlines()]\n",
        "\n",
        "    # Criar arquivo TFRecord\n",
        "    output_path = os.path.join(output_dir, f'{split}.tfrecord')\n",
        "    with tf.io.TFRecordWriter(output_path) as writer:\n",
        "        for image_name in image_names:\n",
        "            image_path = os.path.join(data_dir, 'images', f'{image_name}.jpg')\n",
        "            annotation_path = os.path.join(data_dir, 'labels', f'{image_name}.xml')\n",
        "            if not os.path.exists(annotation_path):\n",
        "                print(f\"Anotação ausente para a imagem: {image_name}\")\n",
        "                continue\n",
        "            tf_example = create_tf_example(image_path, annotation_path, class_map)\n",
        "            writer.write(tf_example.SerializeToString())\n"
      ],
      "metadata": {
        "id": "eoIFeZfCoqXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converter dados para TFRecord\n",
        "convert_to_tfrecord('dataset', 'data', split='train')\n",
        "convert_to_tfrecord('dataset', 'data', split='val')\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n"
      ],
      "metadata": {
        "id": "ca6X9Xb3pb1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Baixar Pesos Pré-treinados\n",
        "# Baixar pesos pré-treinados para inicialização\n",
        "!wget https://pjreddie.com/media/files/yolov3.weights\n",
        "!mv yolov3.weights ./data/\n",
        "!python convert.py --weights ./data/yolov3.weights --output ./checkpoints/yolov3.weights.h5\n",
        "print(\"Preparação concluída! Os dados foram convertidos para TFRecord e os pesos pré-treinados foram baixados.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3tfvJaGpmXE",
        "outputId": "a74c3c9b-1f08-468c-9da9-bc917bcf6f58"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-06 02:23:14--  https://pjreddie.com/media/files/yolov3.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 162.0.215.52\n",
            "Connecting to pjreddie.com (pjreddie.com)|162.0.215.52|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248007048 (237M) [application/octet-stream]\n",
            "Saving to: ‘yolov3.weights’\n",
            "\n",
            "yolov3.weights      100%[===================>] 236.52M  40.0MB/s    in 6.4s    \n",
            "\n",
            "2025-02-06 02:23:20 (36.9 MB/s) - ‘yolov3.weights’ saved [248007048/248007048]\n",
            "\n",
            "2025-02-06 02:23:21.783816: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738808601.813863   36526 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1738808601.821841   36526 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-06 02:23:26.942143: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n",
            "\u001b[1mModel: \"yolov3\"\u001b[0m\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│ input (\u001b[94mInputLayer\u001b[0m)        │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m)  │              \u001b[32m0\u001b[0m │ -                      │\n",
            "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
            "│ yolo_darknet (\u001b[94mFunctional\u001b[0m) │ [(\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,    │     \u001b[32m40,620,640\u001b[0m │ input[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]            │\n",
            "│                           │ \u001b[32m256\u001b[0m), (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,     │                │                        │\n",
            "│                           │ \u001b[96mNone\u001b[0m, \u001b[32m512\u001b[0m), (\u001b[96mNone\u001b[0m,     │                │                        │\n",
            "│                           │ \u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[32m1024\u001b[0m)]     │                │                        │\n",
            "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
            "│ yolo_conv_0 (\u001b[94mFunctional\u001b[0m)  │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,     │     \u001b[32m11,024,384\u001b[0m │ yolo_darknet[\u001b[32m0\u001b[0m][\u001b[32m2\u001b[0m]     │\n",
            "│                           │ \u001b[32m512\u001b[0m)                   │                │                        │\n",
            "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
            "│ yolo_conv_1 (\u001b[94mFunctional\u001b[0m)  │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,     │      \u001b[32m2,957,312\u001b[0m │ yolo_conv_0[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m],     │\n",
            "│                           │ \u001b[32m256\u001b[0m)                   │                │ yolo_darknet[\u001b[32m0\u001b[0m][\u001b[32m1\u001b[0m]     │\n",
            "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
            "│ yolo_conv_2 (\u001b[94mFunctional\u001b[0m)  │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,     │        \u001b[32m741,376\u001b[0m │ yolo_conv_1[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m],     │\n",
            "│                           │ \u001b[32m128\u001b[0m)                   │                │ yolo_darknet[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]     │\n",
            "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
            "│ yolo_output_0             │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m,  │      \u001b[32m4,799,563\u001b[0m │ yolo_conv_0[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]      │\n",
            "│ (\u001b[94mFunctional\u001b[0m)              │ \u001b[32m25\u001b[0m)                    │                │                        │\n",
            "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
            "│ yolo_output_1             │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m,  │      \u001b[32m1,220,171\u001b[0m │ yolo_conv_1[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]      │\n",
            "│ (\u001b[94mFunctional\u001b[0m)              │ \u001b[32m25\u001b[0m)                    │                │                        │\n",
            "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
            "│ yolo_output_2             │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m,  │        \u001b[32m315,211\u001b[0m │ yolo_conv_2[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]      │\n",
            "│ (\u001b[94mFunctional\u001b[0m)              │ \u001b[32m25\u001b[0m)                    │                │                        │\n",
            "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
            "│ yolo_boxes_0 (\u001b[94mLambda\u001b[0m)     │ [(\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, │              \u001b[32m0\u001b[0m │ yolo_output_0[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    │\n",
            "│                           │ \u001b[32m4\u001b[0m), (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, │                │                        │\n",
            "│                           │ \u001b[32m3\u001b[0m, \u001b[32m1\u001b[0m), (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,    │                │                        │\n",
            "│                           │ \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m20\u001b[0m), (\u001b[96mNone\u001b[0m,   │                │                        │\n",
            "│                           │ \u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m4\u001b[0m)]     │                │                        │\n",
            "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
            "│ yolo_boxes_1 (\u001b[94mLambda\u001b[0m)     │ [(\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, │              \u001b[32m0\u001b[0m │ yolo_output_1[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    │\n",
            "│                           │ \u001b[32m4\u001b[0m), (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, │                │                        │\n",
            "│                           │ \u001b[32m3\u001b[0m, \u001b[32m1\u001b[0m), (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,    │                │                        │\n",
            "│                           │ \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m20\u001b[0m), (\u001b[96mNone\u001b[0m,   │                │                        │\n",
            "│                           │ \u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m4\u001b[0m)]     │                │                        │\n",
            "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
            "│ yolo_boxes_2 (\u001b[94mLambda\u001b[0m)     │ [(\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, │              \u001b[32m0\u001b[0m │ yolo_output_2[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]    │\n",
            "│                           │ \u001b[32m4\u001b[0m), (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, │                │                        │\n",
            "│                           │ \u001b[32m3\u001b[0m, \u001b[32m1\u001b[0m), (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,    │                │                        │\n",
            "│                           │ \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m20\u001b[0m), (\u001b[96mNone\u001b[0m,   │                │                        │\n",
            "│                           │ \u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m4\u001b[0m)]     │                │                        │\n",
            "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
            "│ yolo_nms (\u001b[94mLambda\u001b[0m)         │ [(\u001b[32m1\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[32m4\u001b[0m), (\u001b[32m1\u001b[0m,     │              \u001b[32m0\u001b[0m │ yolo_boxes_0[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m],    │\n",
            "│                           │ \u001b[96mNone\u001b[0m), (\u001b[32m1\u001b[0m, \u001b[96mNone\u001b[0m), (\u001b[32m1\u001b[0m)] │                │ yolo_boxes_0[\u001b[32m0\u001b[0m][\u001b[32m1\u001b[0m],    │\n",
            "│                           │                        │                │ yolo_boxes_0[\u001b[32m0\u001b[0m][\u001b[32m2\u001b[0m],    │\n",
            "│                           │                        │                │ yolo_boxes_1[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m],    │\n",
            "│                           │                        │                │ yolo_boxes_1[\u001b[32m0\u001b[0m][\u001b[32m1\u001b[0m],    │\n",
            "│                           │                        │                │ yolo_boxes_1[\u001b[32m0\u001b[0m][\u001b[32m2\u001b[0m],    │\n",
            "│                           │                        │                │ yolo_boxes_2[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m],    │\n",
            "│                           │                        │                │ yolo_boxes_2[\u001b[32m0\u001b[0m][\u001b[32m1\u001b[0m],    │\n",
            "│                           │                        │                │ yolo_boxes_2[\u001b[32m0\u001b[0m][\u001b[32m2\u001b[0m]     │\n",
            "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
            "\u001b[1m Total params: \u001b[0m\u001b[32m61,678,657\u001b[0m (235.29 MB)\n",
            "\u001b[1m Trainable params: \u001b[0m\u001b[32m61,626,049\u001b[0m (235.08 MB)\n",
            "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m52,608\u001b[0m (205.50 KB)\n",
            "I0206 02:23:30.693252 140305330978816 convert.py:27] model created\n",
            "Warning: 248007028 bytes remaining in the file after reading all weights.\n",
            "I0206 02:23:31.027220 140305330978816 convert.py:31] weights loaded\n",
            "I0206 02:23:32.531882 140305330978816 convert.py:36] sanity check passed\n",
            "I0206 02:23:33.919126 140305330978816 convert.py:40] weights saved to ./checkpoints/yolov3.weights.h5\n",
            "Preparação concluída! Os dados foram convertidos para TFRecord e os pesos pré-treinados foram baixados.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv yolov3.weights ./data/"
      ],
      "metadata": {
        "id": "8fTkTSeEB0X5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6b1db06-615b-49a5-982c-16b2049d21a7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat 'yolov3.weights': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py \\\n",
        "    --batch_size 4 \\\n",
        "    --dataset data/train.tfrecord \\\n",
        "    --val_dataset data/val.tfrecord \\\n",
        "    --epochs 10 \\\n",
        "    --mode fit \\\n",
        "    --transfer fine_tune \\\n",
        "    --weights ./checkpoints/yolov3.weights.h5 \\\n",
        "    --num_classes 20 \\\n",
        "    --classes data/voc2007.names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tm-fyHMIzWEU",
        "outputId": "578e1c2c-3be3-49f0-cacc-ae63ab6b7549"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-06 02:37:55.655035: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738809475.692645   40061 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1738809475.701150   40061 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-06 02:38:02.447806: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.11/dist-packages/tensorflow/python/util/deprecation.py:660: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "W0206 02:38:11.368854 134300043358208 deprecation.py:50] From /usr/local/lib/python3.11/dist-packages/tensorflow/python/util/deprecation.py:660: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "2025-02-06 02:38:39.861741: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 88604672 exceeds 10% of free system memory.\n",
            "2025-02-06 02:38:40.343116: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 89031168 exceeds 10% of free system memory.\n",
            "      1/Unknown \u001b[1m43s\u001b[0m 43s/step - loss: 7688.6899 - yolo_output_0_loss: 441.9994 - yolo_output_1_loss: 1868.3856 - yolo_output_2_loss: 5374.23682025-02-06 02:38:51.003232: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 88604672 exceeds 10% of free system memory.\n",
            "2025-02-06 02:38:51.314873: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 89031168 exceeds 10% of free system memory.\n",
            "      2/Unknown \u001b[1m53s\u001b[0m 10s/step - loss: 7882.0078 - yolo_output_0_loss: 508.0006 - yolo_output_1_loss: 1898.7595 - yolo_output_2_loss: 5471.17772025-02-06 02:39:01.385197: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 88604672 exceeds 10% of free system memory.\n",
            "     13/Unknown \u001b[1m155s\u001b[0m 9s/step - loss: 6561.9082 - yolo_output_0_loss: 494.6564 - yolo_output_1_loss: 1586.7660 - yolo_output_2_loss: 4466.4546 /usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n",
            "\n",
            "Epoch 1: saving model to checkpoints/yolov3_train_1.weights.h5\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m181s\u001b[0m 12s/step - loss: 6428.7090 - yolo_output_0_loss: 479.1096 - yolo_output_1_loss: 1542.0297 - yolo_output_2_loss: 4361.7417 - val_loss: 193200.0312 - val_yolo_output_0_loss: 4774.7646 - val_yolo_output_1_loss: 14716.0488 - val_yolo_output_2_loss: 127173.7344 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - loss: 900.4030 - yolo_output_0_loss: 25.5767 - yolo_output_1_loss: 137.3663 - yolo_output_2_loss: 732.3536 \n",
            "Epoch 2: saving model to checkpoints/yolov3_train_2.weights.h5\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 11s/step - loss: 887.0587 - yolo_output_0_loss: 25.2227 - yolo_output_1_loss: 134.3194 - yolo_output_2_loss: 718.1350 - val_loss: 250054.6719 - val_yolo_output_0_loss: 9903.1875 - val_yolo_output_1_loss: 10145.4082 - val_yolo_output_2_loss: 166769.6562 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10s/step - loss: 340.8651 - yolo_output_0_loss: 18.1824 - yolo_output_1_loss: 39.8996 - yolo_output_2_loss: 278.2701 \n",
            "Epoch 3: saving model to checkpoints/yolov3_train_3.weights.h5\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 11s/step - loss: 338.6516 - yolo_output_0_loss: 18.0447 - yolo_output_1_loss: 39.5514 - yolo_output_2_loss: 274.8669 - val_loss: 224254.4219 - val_yolo_output_0_loss: 2979.1689 - val_yolo_output_1_loss: 15356.7861 - val_yolo_output_2_loss: 150632.5156 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10s/step - loss: 218.0215 - yolo_output_0_loss: 16.4152 - yolo_output_1_loss: 27.9840 - yolo_output_2_loss: 169.1568 \n",
            "Epoch 4: saving model to checkpoints/yolov3_train_4.weights.h5\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 16s/step - loss: 217.1192 - yolo_output_0_loss: 16.3723 - yolo_output_1_loss: 27.6638 - yolo_output_2_loss: 167.5378 - val_loss: 35646.7188 - val_yolo_output_0_loss: 1137.5135 - val_yolo_output_1_loss: 4624.0620 - val_yolo_output_2_loss: 20274.1406 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10s/step - loss: 161.5651 - yolo_output_0_loss: 16.4183 - yolo_output_1_loss: 19.8610 - yolo_output_2_loss: 120.8641 \n",
            "Epoch 5: saving model to checkpoints/yolov3_train_5.weights.h5\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 16s/step - loss: 161.0825 - yolo_output_0_loss: 16.2942 - yolo_output_1_loss: 19.7870 - yolo_output_2_loss: 119.8012 - val_loss: 13193.4893 - val_yolo_output_0_loss: 312.6235 - val_yolo_output_1_loss: 1693.2725 - val_yolo_output_2_loss: 7897.4497 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10s/step - loss: 137.0769 - yolo_output_0_loss: 16.5983 - yolo_output_1_loss: 19.0162 - yolo_output_2_loss: 97.1391 \n",
            "Epoch 6: saving model to checkpoints/yolov3_train_6.weights.h5\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 17s/step - loss: 136.6498 - yolo_output_0_loss: 16.6120 - yolo_output_1_loss: 18.8294 - yolo_output_2_loss: 96.3083 - val_loss: 13069.1035 - val_yolo_output_0_loss: 170.2059 - val_yolo_output_1_loss: 711.0356 - val_yolo_output_2_loss: 9509.6465 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - loss: 119.9838 - yolo_output_0_loss: 20.3826 - yolo_output_1_loss: 15.7617 - yolo_output_2_loss: 79.4849 \n",
            "Epoch 7: saving model to checkpoints/yolov3_train_7.weights.h5\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 12s/step - loss: 119.8323 - yolo_output_0_loss: 20.2012 - yolo_output_1_loss: 15.7442 - yolo_output_2_loss: 78.9995 - val_loss: 3794.5032 - val_yolo_output_0_loss: 318.3834 - val_yolo_output_1_loss: 353.1481 - val_yolo_output_2_loss: 2174.1426 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10s/step - loss: 106.4820 - yolo_output_0_loss: 17.3863 - yolo_output_1_loss: 15.5498 - yolo_output_2_loss: 69.1352 \n",
            "Epoch 8: saving model to checkpoints/yolov3_train_8.weights.h5\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 12s/step - loss: 106.4207 - yolo_output_0_loss: 17.1756 - yolo_output_1_loss: 15.4791 - yolo_output_2_loss: 68.8419 - val_loss: 1476.0707 - val_yolo_output_0_loss: 62.4343 - val_yolo_output_1_loss: 452.4951 - val_yolo_output_2_loss: 459.6032 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - loss: 95.8709 - yolo_output_0_loss: 14.4273 - yolo_output_1_loss: 13.9401 - yolo_output_2_loss: 63.1315\n",
            "Epoch 9: saving model to checkpoints/yolov3_train_9.weights.h5\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 11s/step - loss: 95.7965 - yolo_output_0_loss: 14.3707 - yolo_output_1_loss: 13.9927 - yolo_output_2_loss: 62.6345 - val_loss: 591.1819 - val_yolo_output_0_loss: 42.6167 - val_yolo_output_1_loss: 172.1850 - val_yolo_output_2_loss: 185.3156 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - loss: 87.4942 - yolo_output_0_loss: 18.7509 - yolo_output_1_loss: 11.8020 - yolo_output_2_loss: 52.5093 \n",
            "Epoch 10: saving model to checkpoints/yolov3_train_10.weights.h5\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 11s/step - loss: 87.2770 - yolo_output_0_loss: 18.4081 - yolo_output_1_loss: 11.8812 - yolo_output_2_loss: 52.1357 - val_loss: 322.1954 - val_yolo_output_0_loss: 23.9033 - val_yolo_output_1_loss: 66.1964 - val_yolo_output_2_loss: 138.5692 - learning_rate: 0.0010\n",
            "Total Training Time: 2068.961286306381\n"
          ]
        }
      ]
    }
  ]
}